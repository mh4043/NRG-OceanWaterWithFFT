<!--
    Let’s go over various things you might want to time for performance. We’ll time 3 things:

    The frame rate in frames per second (fps)
    The time spent in JavaScript per frame
    The time spent on the GPU per frame
-->


<body>
    <canvas></canvas>
    <pre id="info"></pre>
</body>
<!-- 
    If we set width and hight on canvas element (canvas’s drawing buffer) and width and height in CSS (canvas’s display size),
    the display size will be the same size as its drawing buffer, so we dont have to have helper functions for resizing canvas in JS.
    The browser takes our canvas’s drawing buffer size and stretches it to canvas’s display size.
-->
<style>
body {
    background-color: wheat;
}
canvas {
    display: block;  /* make the canvas act like a block   */
    width: 600px;
    height: 600px;
    border: 2px solid black;
    background-color: white;
    margin: auto; /* center horizontally */
}
#info {
  position: absolute;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0.5em;
  background-color: rgba(0, 0, 0, 0.8);
  color: white;
}
</style>
<script type="module">
    import GUI from 'https://webgpufundamentals.org/3rdparty/muigui-0.x.module.js';
    // WebGPU is an asynchronous API (JS doesn't wait for main to finish executing, but continues)
    async function start() {
        if (!navigator.gpu) {
            fail('this browser does not support WebGPU');
            return;
        }

        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
            fail('this browser supports webgpu but it appears disabled');
            return;
        }

        const canTimestamp = adapter.features.has('timestamp-query');
        const device = await adapter?.requestDevice({
            requiredFeatures: [
            ...(canTimestamp ? ['timestamp-query'] : []),
            ],
        });

        if (!device) {
            fail('need a browser that supports WebGPU');
            return;
        }

        /*
        const device = await adapter?.requestDevice();
        device.lost.then((info) => {
            console.error(`WebGPU device was lost: ${info.message}`);

            // 'reason' will be 'destroyed' if we intentionally destroy the device.
            if (info.reason !== 'destroyed') {
            // try again
            start();
            }
        });
        */
        
        main(device);
    }
    // We use await before function to wait for function to complete
    // await start();
    start();

    // A random number between [min and max)
    // With 1 argument it will be [0 to min)
    // With no arguments it will be [0 to 1)
    const rand = (min, max) => {
        if (min === undefined) {
            min = 0;
            max = 1;
        } else if (max === undefined) {
            max = min;
            min = 0;
        }
        return min + Math.random() * (max - min);
    };

    function createCircleVertices({
        radius = 1,
        numSubdivisions = 24,
        innerRadius = 0,
        startAngle = 0,
        endAngle = Math.PI * 2,
    } = {}) {
        // 2 triangles per subdivision, 3 verts per tri
        const numVertices = numSubdivisions * 3 * 2;
        // 2 32-bit values for position (xy) and 1 32-bit value for color (rgb_)
        // The 32-bit color value will be written/read as 4 8-bit values
        const vertexData = new Float32Array(numVertices * (2 + 1));
        const colorData = new Uint8Array(vertexData.buffer);

        let offset = 0;
        let colorOffset = 8;
        const addVertex = (x, y, r, g, b) => {
            vertexData[offset++] = x;
            vertexData[offset++] = y;
            offset += 1;  // skip the color
            colorData[colorOffset++] = r * 255;
            colorData[colorOffset++] = g * 255;
            colorData[colorOffset++] = b * 255;
            colorOffset += 9;  // skip extra byte and the position
        };

        const innerColor = [1, 1, 1];
        const outerColor = [0.1, 0.1, 0.1];

        // 2 vertices per subdivision
        //
        // 0--1 4
        // | / /|
        // |/ / |
        // 2 3--5
        for (let i = 0; i < numSubdivisions; ++i) {
            const angle1 = startAngle + (i + 0) * (endAngle - startAngle) / numSubdivisions;
            const angle2 = startAngle + (i + 1) * (endAngle - startAngle) / numSubdivisions;

            const c1 = Math.cos(angle1);
            const s1 = Math.sin(angle1);
            const c2 = Math.cos(angle2);
            const s2 = Math.sin(angle2);

            // first triangle
            addVertex(c1 * radius, s1 * radius, ...outerColor);
            addVertex(c2 * radius, s2 * radius, ...outerColor);
            addVertex(c1 * innerRadius, s1 * innerRadius, ...innerColor);

            // second triangle
            addVertex(c1 * innerRadius, s1 * innerRadius, ...innerColor);
            addVertex(c2 * radius, s2 * radius, ...outerColor);
            addVertex(c2 * innerRadius, s2 * innerRadius, ...innerColor);
        }

        return {
            vertexData,
            numVertices,
        };
    }
    
    const euclideanModulo = (x, a) => x - a * Math.floor(x / a);

    function main(device) {
        /*  
            "?." => optional chaning operator 
            "?." operator accesses an object's property or calls a function. 
            If the object accessed or function called using this operator is undefined or null, 
            the expression short circuits and evaluates to undefined instead of throwing an error.

            If the navigaor.gpu does not exist, adapter becomes undefined. Adapter represents a specific GPU.
            Some systems have multiple GPUs

            If the adapter is undefined, then by using adapter?, the device will also be undefined.

            Both functions are async, so we need to use await.
        */
        /*
        const adapter = await navigator.gpu?.requestAdapter();
        const device = await adapter?.requestDevice();
        if (!device) {
            fail('This browser does not support WebGPU');
            return;
        }
        */
        /*
        if (!navigator.gpu) {
            fail("WebGPU not supported.");
            return;
        }
        const adapter = await navigator.gpu.requestAdapter({
            powerPreference: 'high-performance'
        });
        if (!adapter) {
            fail("Couldn't request WebGPU adapter.");
            return;
        }
        const device = await adapter.requestDevice();
        */

        /*
            Look up the canvas and create a webgpu context for it. 
            This will let us get a texture to render to. 
            That texture will be used to display the canvas in the webpage.

            Prefered canvas format is eiter rgba8unorm or bgra8unorm.
        */
        // Get a WebGPU context from the canvas and configure it
        const canvas = document.querySelector('canvas');
        const context = canvas.getContext('webgpu');
        const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
        context.configure({
            device,
            format: presentationFormat,
        });

        /*
            Create a shader module. 
            A shader module contains one or more shader functions. 
            In our case, we’ll make 1 vertex shader function and 1 fragment shader function.
        */
        const module = device.createShaderModule({
            code: `
            struct Vertex {
                @location(0) position: vec2f,
                @location(1) color: vec4f,
                @location(2) offset: vec2f,
                @location(3) scale: vec2f,
                @location(4) perVertexColor: vec3f,
            };

            struct VSOutput {
                @builtin(position) position: vec4f,
                @location(0) color: vec4f,
            };

            @vertex fn vs(
                vert: Vertex,
            ) -> VSOutput {
                var vsOut: VSOutput;
                vsOut.position = vec4f(
                    vert.position * vert.scale + vert.offset, 0.0, 1.0);
                vsOut.color = vert.color * vec4f(vert.perVertexColor, 1);
                return vsOut;
            }

            @fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
                return vsOut.color;
            }
            `,
        });

        const pipeline = device.createRenderPipeline({
            label: 'per vertex color',
            layout: 'auto',
            vertex: {
            module,
            buffers: [
                {
                arrayStride: 2 * 4 + 4, // 2 floats, 4 bytes each + 4 bytes
                attributes: [
                    {shaderLocation: 0, offset: 0, format: 'float32x2'},  // position
                    {shaderLocation: 4, offset: 8, format: 'unorm8x4'},   // perVertexColor
                ],
                },
                {
                arrayStride: 4, // 4 bytes
                stepMode: 'instance',
                attributes: [
                    {shaderLocation: 1, offset: 0, format: 'unorm8x4'},   // color
                ],
                },
                {
                arrayStride: 4 * 4, // 4 floats, 4 bytes each
                stepMode: 'instance',
                attributes: [
                    {shaderLocation: 2, offset: 0, format: 'float32x2'},  // offset
                    {shaderLocation: 3, offset: 8, format: 'float32x2'},  // scale
                ],
                },
            ],
            },
            fragment: {
            module,
            targets: [{ format: presentationFormat }],
            },
        });

        const kNumObjects = 10000;
        const objectInfos = [];

        // create 2 vertex buffers
        const staticUnitSize =
            4;     // color is 4 bytes
        const changingUnitSize =
            2 * 4 + // offset is 2 32bit floats (4bytes each)
            2 * 4;  // scale is 2 32bit floats (4bytes each)
        const staticVertexBufferSize = staticUnitSize * kNumObjects;
        const changingVertexBufferSize = changingUnitSize * kNumObjects;

        const staticVertexBuffer = device.createBuffer({
            label: 'static vertex for objects',
            size: staticVertexBufferSize,
            usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
        });

        const changingVertexBuffer = device.createBuffer({
            label: 'changing storage for objects',
            size: changingVertexBufferSize,
            usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
        });

        // offsets to the various uniform values in float32 indices
        const kColorOffset = 0;

        const kOffsetOffset = 0;
        const kScaleOffset = 2;

        {
            const staticVertexValuesU8 = new Uint8Array(staticVertexBufferSize);
            for (let i = 0; i < kNumObjects; ++i) {
            const staticOffsetU8 = i * staticUnitSize;

            // These are only set once so set them now
            staticVertexValuesU8.set(        // set the color
                [rand() * 255, rand() * 255, rand() * 255, 255],
                staticOffsetU8 + kColorOffset);

            objectInfos.push({
                scale: rand(0.2, 0.5),
                offset: [rand(-0.9, 0.9), rand(-0.9, 0.9)],
                velocity: [rand(-0.1, 0.1), rand(-0.1, 0.1)],
            });
            }
            device.queue.writeBuffer(staticVertexBuffer, 0, staticVertexValuesU8);
        }

        // a typed array we can use to update the changingStorageBuffer
        const vertexValues = new Float32Array(changingVertexBufferSize / 4);

        const { vertexData, numVertices } = createCircleVertices({
            radius: 0.5,
            innerRadius: 0.25,
        });
        const vertexBuffer = device.createBuffer({
            label: 'vertex buffer vertices',
            size: vertexData.byteLength,
            usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
        });
        device.queue.writeBuffer(vertexBuffer, 0, vertexData);

        const renderPassDescriptor = {
            label: 'our basic canvas renderPass',
            colorAttachments: [
            {
                // view: <- to be filled out when we render
                clearValue: [0.3, 0.3, 0.3, 1],
                loadOp: 'clear',
                storeOp: 'store',
            },
            ],
        };

        const infoElem = document.querySelector('#info');

        const settings = {
            numObjects: 100,
        };

        const gui = new GUI();
        gui.add(settings, 'numObjects', 0, kNumObjects, 1);

        const euclideanModulo = (x, a) => x - a * Math.floor(x / a);

        const canvasToSizeMap = new WeakMap();

        function resizeCanvasToDisplaySize(canvas) {
            // Get the canvas's current display size
            let { width, height } = canvasToSizeMap.get(canvas) || canvas;

            // Make sure it's valid for WebGPU
            width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
            height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));

            // Only if the size is different, set the canvas size
            const needResize = canvas.width !== width || canvas.height !== height;
            if (needResize) {
                canvas.width = width;
                canvas.height = height;
            }
            return needResize;
        }

        // Render
        let then = 0;
        function render(now) {
            now *= 0.001;  // convert to seconds
            const deltaTime = now - then;
            then = now;

            const startTime = performance.now();
            // Set the uniform values in our JavaScript side Float32Array
            /*
            const aspect = canvas.width / canvas.height;
            uniformValues.set([0.5 / aspect, 0.5], kScaleOffset); // set the scale
        
            // copy the values from JavaScript to the GPU
            device.queue.writeBuffer(uniformBuffer, 0, uniformValues);
            */

            resizeCanvasToDisplaySize(canvas);

            // Get the current texture from the canvas context and
            // set it as the texture to render to.
            renderPassDescriptor.colorAttachments[0].view = context.getCurrentTexture().createView();

            const encoder = device.createCommandEncoder();
            const pass = encoder.beginRenderPass(renderPassDescriptor);
            pass.setPipeline(pipeline);
            pass.setVertexBuffer(0, vertexBuffer);
            pass.setVertexBuffer(1, staticVertexBuffer);
            pass.setVertexBuffer(2, changingVertexBuffer);

            // Set the uniform values in our JavaScript side Float32Array
            const aspect = canvas.width / canvas.height;

            // set the scales for each object
            for (let ndx = 0; ndx < settings.numObjects; ++ndx) {
            const {scale, offset, velocity} = objectInfos[ndx];
            // -1.5 to 1.5
            offset[0] = euclideanModulo(offset[0] + velocity[0] * deltaTime + 1.5, 3) - 1.5;
            offset[1] = euclideanModulo(offset[1] + velocity[1] * deltaTime + 1.5, 3) - 1.5;

            const off = ndx * (changingUnitSize / 4);
            vertexValues.set(offset, off + kOffsetOffset);
            vertexValues.set([scale / aspect, scale], off + kScaleOffset);
            }

            // upload all offsets and scales at once
            device.queue.writeBuffer(
                changingVertexBuffer, 0,
                vertexValues, 0, settings.numObjects * changingUnitSize / 4);

            pass.draw(numVertices, settings.numObjects);

            pass.end();

            const commandBuffer = encoder.finish();
            device.queue.submit([commandBuffer]);

            const jsTime = performance.now() - startTime;
        
            infoElem.textContent = `\
        fps: ${(1 / deltaTime).toFixed(1)}
        js: ${jsTime.toFixed(1)}ms
        `;

            requestAnimationFrame(render);
        }
    
        requestAnimationFrame(render);

        /*
            <canvas> tags, by default, have a resolution of 300x150 pixels. 
            We’d like to adjust the resolution of the canvas to match the size it is displayed.
            create a ResizeObserver and give it a function to call whenever the elements you’ve asked it to observe change their size. 
            You then tell it which elements to observe.

            We go through all entries, but it should only be one => our one canvas. 
            We need to limit the size of the canvas to the largest size our device supports otherwise WebGPU will start generating errors that we tried to make a texture that is too large. 
            We also need to make sure it doesn’t go to zero or again we’ll get errors.
            
            We call render to re-render the triangle at the new resolution.

            The new size texture is created when we call context.getCurrentTexture() inside render
        */
        const observer = new ResizeObserver(entries => {
            for (const entry of entries) {
                canvasToSizeMap.set(entry.target, {
                    width: entry.contentBoxSize[0].inlineSize,
                    height: entry.contentBoxSize[0].blockSize,
                });
            }
        });
        observer.observe(canvas);
    }

    function fail(msg) {
    // eslint-disable-next-line no-alert
        alert(msg);
    }
</script>